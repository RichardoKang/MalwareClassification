import sys
import os

from tqdm import tqdm
import joblib
from codes.features import *
from codes.utils import *

from sklearn.feature_extraction.text import TfidfVectorizer

from gensim.models import Word2Vec, KeyedVectors
from gensim.models.word2vec import PathLineSentences


def feature_engine(obj, sample_path, inter_path):
    """ Feature engineering for different features. """

    # dirs为['data', 'raw_data', 'train', 'pe']
    dirs = sample_path.split('/')
    # data_type为'train', file_type为'pe'，为什么是-2和-1呢？因为dirs是一个列表，-1表示最后一个元素，-2表示倒数第二个元素
    data_type, file_type = dirs[-2], dirs[-1]
    # 读取useful_filename.txt文件
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        # filename为useful_filename.txt文件中的所有文件名
        filename = fp.read().split()
    # 创建一个全为0的数组，数组的行数为filename的长度，列数为obj的维度
    arr = np.zeros((len(filename), obj.dim))

    if file_type == 'pe':
        # tqdm是一个进度条库，用于显示循环的进度
        with tqdm(total=len(filename), ncols=80, desc=f"{data_type}_{obj.name}") as pbar:
            # i为0到len(filename)的循环，sample为filename中的每一个文件名
            for i, sample in enumerate(filename):
                # 打开文件
                with open(f"{sample_path}/{sample}", "rb") as f:
                    # 读取文件的字节
                    bytez = f.read()
                # 将文件的字节转化为特征向量
                arr[i, :] = obj.feature_vector(bytez)
                # 更新进度条
                pbar.update(1)
    else:  # file_type == 'asm'
        with tqdm(total=len(filename), ncols=80, desc=f"{data_type}_{obj.name}") as pbar:
            for i, sample in enumerate(filename):
                # 打开文件
                with open(f"{sample_path}/{sample}.asm", "rb") as f:
                    # 读取文件的内容，解码为utf-8格式
                    stringz = f.read().decode('utf-8', errors='ignore')
                # 将文件的内容转化为特征向量
                arr[i, :] = obj.feature_vector(stringz)
                # 更新进度条
                pbar.update(1)
    # 将arr保存为npy文件
    np.save(f"{inter_path}/feature/{data_type}_{obj.name}.npy", arr)


def feature_tfidf_df(obj, sample_path, inter_path):
    """ Save the words of all samples to a DataFrame fot tf-idf input. """
    dirs = sample_path.split('/')
    data_type, file_type = dirs[-2], dirs[-1]
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    if file_type == 'asm':
        filename = [f + '.asm' for f in filename]
    all_word_feature = []
    with tqdm(total=len(filename), ncols=80, desc=f"{obj.name_tfidf}_{data_type}") as pbar:
        for i, sample in enumerate(filename):
            with open(f"{sample_path}/{sample}", "r", encoding='utf-8', errors='ignore') as f:
                all_word_feature.append(obj.tfidf_features(f))
            pbar.update(1)

    # 将all_word_feature保存为csv文件
    word_feature = pd.DataFrame({'filename':filename, "word_feature": all_word_feature})
    word_feature.to_csv(f"{inter_path}/feature/{data_type}_{obj.name_tfidf}_tfidf.csv", index=False)

    # return word_feature


def model_tfidf(obj, sample_path, inter_path, tfidf_params):
    """ Save the tf-idf model """
    # 读取train_words_和test_words_的word_feature
    train_words_ = pd.read_csv(f"{inter_path}/feature/train_{obj.name_tfidf}_tfidf.csv")
    delect789 = list(np.load(f"{inter_path}/train_filename_de.npy")) # 不要训练集的789 太少了
    # 删除train_words_中的文件名在delect789中的行
    train_words_de = train_words_.loc[~train_words_.filename.isin(delect789)]

    # 读取test_words_的word_feature
    test_words_ = pd.read_csv(f"{inter_path}/feature/test_{obj.name_tfidf}_tfidf.csv")

    all_words_ = train_words_de.append(test_words_)

    # 生成tf-idf模型
    vectorizer = TfidfVectorizer(**tfidf_params)
    # 将all_words_的word_feature转化为列表，然后拟合模型
    vectorizer.fit(all_words_.word_feature.tolist())
    # 把模型保存为pth文件
    joblib.dump(vectorizer, open(f"{inter_path}/models/TFIDF_model_{obj.name_tfidf}_{tfidf_params['max_features']}.pth", "wb"))


def feature_tfidf_np(data_type, name_tfidf, inter_path, max_features):
    """ Save the tf-idf feature with numpy """
    # 读取TFIDF_model的模型
    vectorizer = joblib.load(open(f"{inter_path}/models/TFIDF_model_{name_tfidf}_{max_features}.pth", "rb"))
    # 设置最大特征数
    vectorizer.max_features = max_features
    # 读取train_words_和test_words_的word_feature
    words_ = pd.read_csv(f"{inter_path}/feature/{data_type}_{name_tfidf}_tfidf.csv")
    # 将word_feature转化为列表，然后转化为数组
    words = vectorizer.transform(words_.word_feature.tolist())
    # 将数组保存为npy文件
    np.save(f"{inter_path}/feature/{data_type}_{name_tfidf}_{max_features}.npy", words.toarray())


def feature_asm2txt(sample_path, inter_path):
    """ Save the opcode of all samples to a txt for asm2vec input. """
    dirs = sample_path.split('/')
    data_type, file_type = dirs[-2], dirs[-1]

    def asm2txt_by_datatype(spath, dtype):
        with open(f"{inter_path}/{dtype}_filename.txt", 'r') as fp:
            filenames = fp.read().split()

        if dtype == 'test':
            spath = spath.replace('train', 'test')

        with tqdm(total=len(filenames), ncols=80, desc=f"{dtype}_asm2txt") as pbar:
            for filename in filenames:
                with open(os.path.join(spath, filename) + '.asm', "r", encoding='utf-8', errors='ignore') as fp:
                    opline_list = OpcodeInfo().asm_to_txt(fp)
                f = open(os.path.join(f"{inter_path}/semantic/", filename) + '.txt', 'w+', encoding='utf-8', errors='ignore')
                for line in opline_list:
                    f.write(line)
                    f.write('\n')
                f.close()
                pbar.update(1)

    asm2txt_by_datatype(sample_path, data_type)
    asm2txt_by_datatype(sample_path, 'test')


def feature_asm2vec(data_type, inter_path):
    """Feature engineering for asm2vec feature."""

    if data_type == "train":
        # TODO : 模型空判断
        # Train a Word2vec model by mixing traing set and test set
        print("------------------------ 训练asm2vec模型 ------------------------")
        sentences = PathLineSentences(f"{inter_path}/semantic/")
        model = Word2Vec(sentences=sentences, vector_size=1024, window=5, min_count=5, workers=5)
        model.wv.save_word2vec_format(f"{inter_path}/models/asm2vec.bin", binary=True, sort_attr='count')

    # Load the trained Word2vec model
    model_wv = KeyedVectors.load_word2vec_format(f"{inter_path}/models/asm2vec.bin", binary=True)

    print("------------------------ 生成asm2vec特征 ------------------------")
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    # Feature engineering for generating string vector features
    obj = StringVector()
    arr = np.zeros((len(filename), obj.dim))
    with tqdm(total=len(filename), ncols=80, desc=obj.name) as pbar:
        for i, file in enumerate(filename):
            with open(f"{inter_path}/semantic/{file}.txt", "rb") as f:
                stringz = f.read().decode('utf-8', errors='ignore')
            lines = ' '.join(stringz.split('\n'))
            raw_words = list(set(lines.split()))
            arr[i, :] = obj.feature_vector((model_wv, raw_words))
            pbar.update(1)
    arr[np.isnan(arr)] = 0
    np.save(f"{inter_path}/feature/{data_type}_semantic.npy", arr)


def feature_fusion(data_type, fused_label, features, inter_path):

    arr = []
    for f in features:
        arr.append(np.load(f"{inter_path}/feature/{data_type}_{f}.npy"))
    np.save(f"{inter_path}/feature/{data_type}_{fused_label}.npy", np.hstack(arr).astype(np.float32))


def feature_engineering(data_type, data_path, inter_path):

    pe_path = f"{data_path}/{data_type}/pe"
    asm_path = f"{data_path}/{data_type}/asm"

    if data_type == 'train':

        print("------------------------ 生成索引文件 ------------------------")
        # TODO：file_index without under sample
        fix_file_index(data_path, inter_path)

        print("------------------------ 生成TF-IDF词库 ------------------------")
        feature_tfidf_df(StringExtractor(), pe_path, inter_path)
        feature_tfidf_df(StringExtractor(), pe_path.replace('train', 'test'), inter_path)

        feature_tfidf_df(OpcodeInfo(), asm_path, inter_path)
        feature_tfidf_df(OpcodeInfo(), asm_path.replace('train', 'test'), inter_path)

        print("------------------------ 生成TF-IDF模型 ------------------------")
        words_tf_params1 = {'max_features':1000}
        model_tfidf(StringExtractor(), pe_path, inter_path, words_tf_params1)
        words_tf_params2 = {'max_features': 300}
        model_tfidf(StringExtractor(), pe_path, inter_path, words_tf_params2)
        ins_tf_params = {'stop_words' : [';'], 'ngram_range' : (1, 3), 'max_features':1000}
        model_tfidf(OpcodeInfo(), asm_path, inter_path, ins_tf_params)

        print("------------------------ 生成asm2vec词库 ------------------------")
        feature_asm2txt(asm_path, inter_path)


    print("------------------------ 生成TF-IDF特征 ------------------------")
    feature_tfidf_np(data_type, 'words', inter_path, max_features=300)
    feature_tfidf_np(data_type, 'words', inter_path, max_features=1000)
    feature_tfidf_np(data_type, 'ins', inter_path, max_features=1000)

    print("------------------------ 生成ember特征 ------------------------")
    pe_objs = [ByteHistogram(), ByteEntropyHistogram(), StringExtractor()]
    for obj in pe_objs:
        feature_engine(obj, pe_path, inter_path)

    asm_objs = [SectionInfo(), ImportsInfo(), ExportsInfo()]
    for obj in asm_objs:
        feature_engine(obj, asm_path, inter_path)

    # ------------------------ 生成asm2vec特征 ------------------------
    feature_asm2vec(data_type, inter_path)

    print("------------------------ 特征融合 ------------------------")
    feature_fusion(data_type, 'ember', ['histogram', 'byteentropy', 'strings'], inter_path)
    feature_fusion(data_type, 'ember_section_ins_words', ['ember', 'section', 'ins_1000', 'words_300'], inter_path)
    feature_fusion(data_type, 'ember_section_ins_semantic', ['ember', 'section', 'ins_1000', 'semantic'], inter_path)