import sys
import os
import io
from pickletools import OpcodeInfo

import pandas as pd
import numpy as np
from tqdm import tqdm
import joblib
from utils import *

from features_class import StringExtractor

sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf8')
from sklearn.feature_extraction.text import TfidfVectorizer

from gensim.models import Word2Vec, KeyedVectors
from gensim.models.word2vec import PathLineSentences


def feature_engine(obj, sample_path, inter_path):  # sample_path: 'data/train/pe',inter_path: 'data/user_data'
    """ 特征工程 """
    dirs = sample_path.split('/')
    # 数据属于“训练”或“测试”，文件属于“pe”或“asm”
    data_type, file_type = dirs[-2], dirs[-1]
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()
    arr = np.zeros((len(filename), obj.dim))

    if file_type == 'pe':
        with tqdm(total=len(filename), ncols=80, desc=f"{data_type}_{obj.name}") as pbar:
            # 读取文件夹下的所有文件名
            for i, sample in enumerate(filename):
                with open(f"{sample_path}/{sample}", "rb") as f:
                    # 读取文件内容
                    bytez = f.read()
                arr[i, :] = obj.feature_vector(bytez)  # 提取特征
                pbar.update(1)

    else:  # file_type == 'asm'
        with tqdm(total=len(filename), ncols=80, desc=f"{data_type}_{obj.name}") as pbar:
            for i, sample in enumerate(filename):
                with open(f"{sample_path}/{sample}.asm", "rb") as f:
                    stringz = f.read().decode('utf-8', errors='ignore')
                arr[i, :] = obj.feature_vector(stringz)
                pbar.update(1)

    np.save(f"{inter_path}/feature/{data_type}_{obj.name}.npy", arr)


def feature_tfidf_df(obj, sample_path, inter_path):
    """ 把TF-IDF特征保存为dataframe """
    dirs = sample_path.split('/')  # 把路径分割成目录和文件名
    data_type, file_type = dirs[-2], dirs[-1]  # 取出目录和文件名
    with open(f"{inter_path}/{data_type}_filename.txt", 'r') as fp:
        filename = fp.read().split()  # 读取文件名
    if file_type == 'asm':
        filename = [f + '.asm' for f in filename]
    all_word_feature = []
    with tqdm(total=len(filename), ncols=80, desc=f"{obj.name_tfidf}_{data_type}") as pbar:  #
        for i, sample in enumerate(filename):
            with open(f"{sample_path}/{sample}", "r", encoding='utf-8', errors='ignore') as f:
                all_word_feature.append(obj.tfidf_features(f))
            pbar.update(1)

    # 把所有的词语转换为词频矩阵
    word_feature = pd.DataFrame({'filename': filename, "word_feature": all_word_feature})
    word_feature.to_csv(f"{inter_path}/feature/{data_type}_{obj.name_tfidf}_tfidf.csv", index=False)

    # return word_feature


def model_tfidf(obj, sample_path, inter_path, tfidf_params):
    """ 训练TF-IDF模型 """
    train_words_ = pd.read_csv(f"{inter_path}/feature/train_{obj.name_tfidf}_tfidf.csv")
    delect789 = list(np.load(f"{inter_path}/train_filename_de.npy"))  # 不要训练集的789 太少了
    train_words_de = train_words_.loc[~train_words_.filename.isin(delect789)]

    test_words_ = pd.read_csv(f"{inter_path}/feature/test_{obj.name_tfidf}_tfidf.csv")
    all_words_ = train_words_de.append(test_words_)

    vectorizer = TfidfVectorizer(**tfidf_params)
    vectorizer.fit(all_words_.word_feature.tolist())

    joblib.dump(vectorizer,
                open(f"{inter_path}/models/TFIDF_model_{obj.name_tfidf}_{tfidf_params['max_features']}.pth", "wb"))


def feature_tfidf_np(data_type, name_tfidf, inter_path, max_features):
    """ 把TF-IDF特征保存为numpy数组 """
    vectorizer = joblib.load(open(f"{inter_path}/models/TFIDF_model_{name_tfidf}_{max_features}.pth", "rb"))
    vectorizer.max_features = max_features
    words_ = pd.read_csv(f"{inter_path}/feature/{data_type}_{name_tfidf}_tfidf.csv")
    words = vectorizer.transform(words_.word_feature.tolist())
    np.save(f"{inter_path}/feature/{data_type}_{name_tfidf}_{max_features}.npy", words.toarray())


def feature_fusion(data_type, fused_label, features, inter_path):
    """ 特征融合 """
    arr = []
    for f in features:
        arr.append(np.load(f"{inter_path}/feature/{data_type}_{f}.npy"))
    np.save(f"{inter_path}/feature/{data_type}_{fused_label}.npy", np.hstack(arr).astype(np.float32))


def file_index(pe_path, inter_path):
    """ 生成索引文件 """
    with open(f"{inter_path}/train_filename.txt", 'w') as fp:
        for filename in os.listdir(pe_path):
            fp.write(filename.split('.')[0] + '\n')


def feature_engineering(data_type, data_path, inter_path):
    pe_path = f"{data_path}/{data_type}/pe"
    asm_path = f"{data_path}/{data_type}/asm"

    if data_type == 'train':
        print("------------------------ 生成索引文件 ------------------------")
        # TODO：file_index without under sample
        fix_file_index(data_path, inter_path)

        print("------------------------ 生成TF-IDF词库 ------------------------")
        feature_tfidf_df(StringExtractor(), pe_path, inter_path)
        feature_tfidf_df(StringExtractor(), pe_path.replace('train', 'test'), inter_path)

        feature_tfidf_df(OpcodeInfo(), asm_path, inter_path)
        feature_tfidf_df(OpcodeInfo(), asm_path.replace('train', 'test'), inter_path)

        print("------------------------ 生成TF-IDF模型 ------------------------")
        words_tf_params1 = {'max_features': 1000}
        model_tfidf(StringExtractor(), pe_path, inter_path, words_tf_params1)
        words_tf_params2 = {'max_features': 300}
        model_tfidf(StringExtractor(), pe_path, inter_path, words_tf_params2)
        ins_tf_params = {'stop_words': [';'], 'ngram_range': (1, 3), 'max_features': 1000}
        model_tfidf(OpcodeInfo(), asm_path, inter_path, ins_tf_params)
        """
        print("------------------------ 生成asm2vec词库 ------------------------")
        feature_asm2txt(asm_path, inter_path)
        """

    print("------------------------ 生成TF-IDF特征 ------------------------")
    feature_tfidf_np(data_type, 'words', inter_path, max_features=300)
    feature_tfidf_np(data_type, 'words', inter_path, max_features=1000)
    feature_tfidf_np(data_type, 'ins', inter_path, max_features=1000)

    """
    print("------------------------ 生成ember特征 ------------------------")
    pe_objs = [ByteHistogram(), ByteEntropyHistogram(), StringExtractor()]
    for obj in pe_objs:
        feature_engine(obj, pe_path, inter_path)

    asm_objs = [SectionInfo(), ImportsInfo(), ExportsInfo()]
    for obj in asm_objs:
        feature_engine(obj, asm_path, inter_path)

    # ------------------------ 生成asm2vec特征 ------------------------
    feature_asm2vec(data_type, inter_path)
    """

    print("------------------------ 特征融合 ------------------------")
    feature_fusion(data_type, 'ember', ['histogram', 'byteentropy', 'strings'], inter_path)
    feature_fusion(data_type, 'ember_section_ins_words', ['ember', 'section', 'ins_1000', 'words_300'], inter_path)
    feature_fusion(data_type, 'ember_section_ins_semantic', ['ember', 'section', 'ins_1000', 'semantic'], inter_path)
